{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zpUHZpbYpn7"
      },
      "source": [
        "# Laboratory Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Instruction:** Perform a forward and backward propagation in python using the inputs from Laboratory Task 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5gT6R_hX1sb"
      },
      "source": [
        "**Import Library**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "g7TDNPizTX_x"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tqf3aXHYJZt"
      },
      "source": [
        "**Inititalize network parameters and inputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WqC4gDAyXy3K"
      },
      "outputs": [],
      "source": [
        "x = np.array([1,0,1]).reshape(3,1)\n",
        "y = np.array([1]).reshape(1,1)\n",
        "lr = 0.001\n",
        "\n",
        "hwu = np.array([[0.2,-0.3], [0.4, 0.1], [-0.5, 0.2]])\n",
        "hwub = np.array([[-0.4], [0.2]])\n",
        "ouw = np.array([[-0.3, -0.2]])\n",
        "owub = np.array([[0.1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E23uu2dbYODa"
      },
      "source": [
        "**Forward Pass**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnUjXE0rYMbK",
        "outputId": "bb12b272-f4c1-433b-80b7-3ad26acf41b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Output: 0.0800\n",
            "Error: -0.9200\n"
          ]
        }
      ],
      "source": [
        "# Define the ReLU activation function and derivative\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "# Calculate weighted sum for hidden layer\n",
        "wsh = np.dot(hwu.T, x) + hwub\n",
        "hidden = relu(wsh)\n",
        "\n",
        "# Calculate weighted sum for output layer\n",
        "wso = np.dot(owu, hidden) + owub\n",
        "pred_output = wso # No activation function on the output, as per the manual calculation\n",
        "\n",
        "# Calculate the error (MSE is commonly used, but for simplicity here we use the difference)\n",
        "error = pred_output - y\n",
        "print(f\"Predicted Output: {pred_output[0][0]:.4f}\")\n",
        "print(f\"Error: {error[0][0]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3v73FE3YUxs"
      },
      "source": [
        "**Backward Propagation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwLebrf4YSDB",
        "outputId": "76add041-b856-4a79-a8c9-e33b5ebf533c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- After one step of training ---\n",
            "Updated Hidden Weights (W_h):\n",
            "[[ 0.2      -0.300184]\n",
            " [ 0.4       0.1     ]\n",
            " [-0.5       0.199816]]\n",
            "\n",
            "Updated Hidden Biases (b_h):\n",
            "[[-0.4     ]\n",
            " [ 0.199816]]\n",
            "\n",
            "Updated Output Weights (W_o):\n",
            "[[-0.3      -0.199908]]\n",
            "\n",
            "Updated Output Bias (b_o):\n",
            "[[0.10092]]\n"
          ]
        }
      ],
      "source": [
        "# Calculate gradients for the output layer\n",
        "delta_o = error # The derivative of the loss with respect to the output is (y_hat - y)\n",
        "d_Wo = np.dot(delta_o, hidden.T)\n",
        "d_bo = delta_o\n",
        "\n",
        "# Calculate gradients for the hidden layer\n",
        "delta_h = np.dot(owu.T, delta_o) * relu_derivative(wsh)\n",
        "d_Wh = np.dot(x, delta_h.T)\n",
        "d_bh = delta_h\n",
        "\n",
        "owu -= lr * d_Wo\n",
        "owub -= lr * d_bo\n",
        "hwu -= lr * d_Wh\n",
        "hwub -= lr * d_bh\n",
        "\n",
        "print(\"\\n--- After one step of training ---\")\n",
        "print(f\"Updated Hidden Weights (W_h):\\n{hwu}\")\n",
        "print(f\"\\nUpdated Hidden Biases (b_h):\\n{hwub}\")\n",
        "print(f\"\\nUpdated Output Weights (W_o):\\n{owu}\")\n",
        "print(f\"\\nUpdated Output Bias (b_o):\\n{owub}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
